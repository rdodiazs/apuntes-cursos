\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{amsmath, amssymb}
\usepackage{tikz}
\usepackage{hyperref} % Siempre debe ir al final.

% Opciones de Paquetes.
\decimalpoint          % {babel}
\onehalfspacing        % {setspace}
\usetikzlibrary{babel} % {tikz}: Para que tikz no conflictue con {babel} con figuras como "->".

\title{Clase 3. Matrices y la Matriz Inversa.}
\author{MIT 18.02: Multivariable Calculus.}
\date{}


\begin{document}

% Comandos personalizados.
\newcommand{\vecmat}[1]{\mathbf{#1}}                          % Vectores o matrices en negrita en math mode.
\newcommand{\unitvec}[1]{\vecmat{\hat{#1}}}                   % Vectores unitarios.
\newcommand{\overvec}[1]{\overrightarrow{#1}}                 % Vector como segmento orientado.
\newcommand{\invmat}[1]{\vecmat{#1}^{-1}}                     % Inversa de una matriz.
\newcommand{\transmat}[1]{\vecmat{#1}^{T}}                    % Transpuesta de una matriz.
\newcommand{\Adj}[0]{\text{Adj}}                              % Matriz adjunta.
\newcommand{\R}[0]{\mathbb{R}}                                % Símbolo conjunto de los números reales.
\newcommand{\N}[0]{\mathbb{N}}                                % Símbolo conjunto de los números naturales.

\maketitle

\begin{abstract}
\noindent En esta clase estudiaremos un nuevo objeto matemático conocido como \textbf{matriz}. Veremos sus propiedades, operaciones básicas y nos concentraremos en un tipo de ellas llamada \textbf{matriz inversa}.
\end{abstract}


\section{Matrices.}

Considere un punto $P$ con coordenadas $(x_{1}, \ x_{2}, \ x_{3})$ en un espacio cartesiano. Luego, suponga que definimos los ejes de este sistema para una nueva variable $u$, implicando que $P$ queda ubicado en $(u_{1}, \ u_{2}, \ u_{3})$ donde:
\[
  u_{1} = 2x_{1} + 3x_{2} + 3x_{3}, \qquad
  u_{2} = 2x_{1} + 4x_{2} + 5x_{3}, \qquad
  u_{3} =  x_{1} + x_{2} + 2x_{3}
\]
Asumamos que conocemos a $(u_{1}, \ u_{2}, \ u_{3})$, pero no a $(x_{1}, \ x_{2}, \ x_{3})$. Esta última coordenada se puede obtener resolviendo las igualdades de arriba como un sistema de ecuaciones lineales.
\[
\left\{
\begin{aligned}
2x_{1} + 3x_{2} + 3x_{3} &= u_{1} \\
2x_{1} + 4x_{2} + 5x_{3} &= u_{2} \\
  x_{1} + x_{2} + 2x_{3} &= u_{3}
\end{aligned}
\right.
\]
Es posible compactar este sistema mediante la siguiente \textbf{ecuación matricial}:
\[
  \vecmat{A} \vecmat{x} = \vecmat{u},
\]
donde $\vecmat{A}$ es la \textbf{matriz de coeficientes}, puesto que contiene a las constantes (coeficientes) del sistema; mientras que $\vecmat{x}$ y $\vecmat{u}$ reciben el nombre de \textbf{vectores columna}.

\newpage

\begin{figure}[hbt!]
\centering

\begin{tikzpicture}
% Líneas de ayuda.
%\draw[color = lightgray] (0, 0) grid (14, 4);

% Matrices y vectores.
\node at (2.5, 3.2) {Matriz de coeficientes};
\node at (2, 2) {
$
\displaystyle
\vecmat{A} =
\begin{bmatrix}
2 & 3 & 3 \\
2 & 4 & 5 \\
1 & 1 & 2
\end{bmatrix}
$
};

\node at (8.85, 3.2) {Vectores columna};
\node at (7, 2) {
$
\displaystyle
\vecmat{x} =
\begin{bmatrix}
x_{1} \\x_{2} \\ x_{3}
\end{bmatrix}
$
};
\node at (10, 2) {
$
\displaystyle
\vecmat{u} =
\begin{bmatrix}
u_{1} \\u_{2} \\ u_{3}
\end{bmatrix}
$
};
\end{tikzpicture}
\end{figure}

En este curso entenderemos a las \textbf{matrices} como arreglos rectangulares de números, símbolos o expresiones ordenados en filas y columnas.

Es común denotar a las matrices con letras mayúsculas en negrita. Anteriormente lo hicimos con $\vecmat{A}$. Por otra parte, cuando sus entradas son expresadas algebraicamente, se escriben en minúscula y con dos subíndices que indican su posición en ella.
\[
  a_{3,5} \Longrightarrow \text{Entrada de } \vecmat{A} \text{ ubicada en su tercera fila y quinta columna.}
\]
Al igual que con los vectores, las matrices también pueden ser expresadas indicando su \textbf{dimensión}, la que señala la cantidad de filas y columnas como un producto.
\[
\vecmat{A} =
\begin{bmatrix}
2 & 1 & 3 \\
1 & 0 & 4
\end{bmatrix}
\Longrightarrow
\text{Dimensión de } \vecmat{A}: \ 2 \times 3
\Longrightarrow
2 \text{ filas y } 3 \text{ columnas.}
\]
Otra forma de expresar la dimensión de una matriz es añadiendo el conjunto numérico del cual provienen sus entradas. En el ejemplo de arriba, $\vecmat{A} \in \R^{2 \times 3}$.

Para toda matriz $\vecmat{A}$ de $n \times m$, cuando $n = m$ se dice que es \textbf{cuadrada} y si $n \neq m$ se etiqueta como \textbf{rectangular}.

Es posible interpretar a los vectores como matrices. Por ejemplo,
\[
\vecmat{x} =
\begin{bmatrix}
x_{1} \\x_{2} \\ x_{3}
\end{bmatrix} \qquad
\vecmat{u} =
\begin{bmatrix}
u_{1} \\u_{2} \\ u_{3}
\end{bmatrix}
\]
pueden ser expresados como matrices de $3 \times 1$.

Por otra parte, las columnas y filas de una matriz pueden ser entendidas como vectores. Las primeras se nombran como \textbf{vectores columna} y las segundas como \textbf{vectores fila}. Estas últimas suelen ser de la forma:
\[
\vecmat{c} =
\begin{bmatrix}
c_{1} & c_{2} & c_{3}
\end{bmatrix}
\]

Dos o más \textbf{matrices} son \textbf{iguales} si sus \textbf{dimensiones y entradas} también lo son.

\subsection{Operaciones básicas con matrices.}

La adición matricial y la multiplicación escalar se calculan de forma similar a cómo se realizan dichas operaciones con vectores.

Si dos matrices $\vecmat{V}$ y $\vecmat{W}$ tienen la misma dimensión, entonces la \textbf{suma} entre ambas se define como:
\[
\vecmat{V} \pm \vecmat{W} =
\begin{bmatrix}
v_{1, 1} \pm w_{1, 1} & v_{1, 2} \pm w_{1, 2} & \cdots & v_{1, m} \pm w_{1, m} \\
v_{2, 1} \pm w_{2, 1} & v_{2, 2} \pm w_{2, 2} & \cdots & v_{2, m} \pm w_{2, m} \\
\vdots & \vdots & \ddots & \vdots \\
v_{n, 1} \pm w_{n, 1} & v_{n, 2} \pm w_{n, 2} & \cdots & v_{n, m} \pm w_{n, m}
\end{bmatrix}
\]
Por ejemplo:
\[
\begin{bmatrix}
1 & 4 \\
3 & 5
\end{bmatrix} +
\begin{bmatrix}
2 & 1 \\
9 & 6
\end{bmatrix} =
\begin{bmatrix}
1 + 2 & 4 + 1 \\
3 + 9 & 5 + 6
\end{bmatrix} =
\begin{bmatrix}
3 & 5 \\
12 & 11
\end{bmatrix}
\]
Sea $k$ un número. La \textbf{multiplicación escalar} entre este valor y una matriz $\vecmat{V}$ es:
\[
k \cdot \vecmat{V} =
\begin{bmatrix}
k \cdot v_{1,1} & k \cdot v_{1, 2} & \cdots & k \cdot v_{1, m} \\
k \cdot v_{2,1} & k \cdot v_{2, 2} & \cdots & k \cdot v_{2, m} \\
\vdots & \vdots & \ddots & \vdots \\
k \cdot v_{n, 1} & k \cdot v_{n, 2} & \cdots & k \cdot v_{n, m}
\end{bmatrix}
\]

\subsection{Producto matricial.}

Otra operación relevante entre matrices es el \textbf{producto matricial}. Comúnmente se realiza entre dos de ellas y, para aplicarla, debe cumplirse la siguiente condición:

\begin{quote}
\centering
\textit{La cantidad de columnas de la matriz de la izquierda debe ser igual a la cantidad de filas de la matriz de la derecha}.
\end{quote}

Cuando se cumple esta condición, es posible calcular el producto matricial entre dos matrices como el \textbf{producto punto} entre los vectores fila de la que está a la izquierda y los vectores columna de la que se encuentra en la derecha, la que resulta en una nueva matriz.

A modo de ejemplo, considere las siguientes matrices:
\[
\vecmat{A} =
\begin{bmatrix}
1 & 3 & 2 \\
0 & -1 & 5
\end{bmatrix}
\qquad
\vecmat{B} =
\begin{bmatrix}
0 & 3 \\
2 & 1 \\
4 & -5
\end{bmatrix}
\]
Debido a que $\vecmat{A} \in \R^{2 \times 3}$ y $\vecmat{B} \in \R^{3 \times 2}$, es posible calcular el producto entre ambas como:
\begin{align*}
\vecmat{A} \cdot \vecmat{B} &=
\begin{bmatrix}
1 & 3 & 2 \\
0 & -1 & 5
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & 3 \\
2 & 1 \\
4 & -5
\end{bmatrix} \\
&=
\begin{bmatrix}
(1 \cdot 0) + (3 \cdot 2) + (2 \cdot 4) & (1 \cdot 3) + (3 \cdot 1) + (2 \cdot -5) \\
(0 \cdot 0) + (-1 \cdot 2) + (5 \cdot 4) & (3 \cdot 0) + (-1 \cdot 1) + (5 \cdot -5)
\end{bmatrix} \\
&=
\begin{bmatrix}
14 & -4 \\
18 & -26
\end{bmatrix}
\end{align*}
Como se observa arriba, $\vecmat{A} \cdot \vecmat{B} \in \R^{2 \times 2}$. En general, si $\vecmat{A}$ es de $n \times k$ dimensiones y $\vecmat{B}$ es de $k \times m$, entonces $\vecmat{A} \cdot \vecmat{B}$ será de $n \times m$.

La multiplicación entre una matriz y un vector también es un producto matricial. Cuando es de la forma
\[
\vecmat{A}\vecmat{x} =
\begin{bmatrix}
a_{1, 1} & a_{1, 2} & \ldots & a_{1, m} \\
a_{2, 1} & a_{2, 2} & \ldots & a_{2, m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n, 1} & a_{n, 2} & \ldots & a_{n, m}
\end{bmatrix}
\begin{bmatrix}
x_{1} \\ x_{2} \\ \vdots \\ x_{m}
\end{bmatrix} =
\begin{bmatrix}
a_{1, 1}x_{1} + a_{1, 2}x_{2} + \ldots + a_{1, m}x_{m} \\
a_{2, 1}x_{1} + a_{2, 2}x_{2} + \ldots + a_{2, m}x_{m} \\
\vdots \\
a_{n, 1}x_{1} + a_{n, 2}x_{2} + \ldots + a_{n, m}x_{m}
\end{bmatrix}
\]
recibe el nombre de \textbf{producto matriz-vector} y, como se observa arriba, resulta en un \textbf{vector}.

\subsubsection{Otras formas de interpretar el producto matricial.}

Consideremos el producto entre dos matrices $\vecmat{A}, \ \vecmat{B} \in \R^{2 \times 2}$.
\[
\vecmat{A} \cdot \vecmat{B} =
\begin{bmatrix}
a_{1, 1} & a_{1, 2} \\
a_{2, 1} & a_{2, 2}
\end{bmatrix} \cdot
\begin{bmatrix}
b_{1, 1} & b_{1, 2} \\
b_{2, 1} & b_{2, 2}
\end{bmatrix} =
\begin{bmatrix}
a_{1, 1}b_{1, 1} + a_{1, 2}b_{2, 1} & a_{1, 1}b_{1, 2} + a_{1, 2}b_{2, 2} \\
a_{2, 1}b_{1, 1} + a_{2, 2}b_{2, 1} & a_{2, 1}b_{1, 2} + a_{2, 2}b_{2, 2}
\end{bmatrix}
\]
Se puede apreciar arriba que cada columna de $\vecmat{A} \cdot \vecmat{B}$ corresponde al producto matriz-vector entre $\vecmat{A}$ y los vectores columna de $\vecmat{B}$, $\vecmat{b}_{i}$.
\[
\vecmat{A} \cdot \vecmat{B} =
\begin{bmatrix}
 & \\
\vecmat{A} \cdot \vecmat{b}_{1} & \vecmat{A} \cdot \vecmat{b}_{2} \\
 & 
\end{bmatrix}; \text{ donde }
\vecmat{b}_{1} =
\begin{bmatrix}
b_{1, 1} \\ b_{2, 1}
\end{bmatrix} \text{ y }
\vecmat{b}_{2} =
\begin{bmatrix}
b_{1, 2} \\ b_{2, 2}
\end{bmatrix}
\]
Por lo tanto, si $\vecmat{C} = \vecmat{A} \cdot \vecmat{B}$, para $\vecmat{A} \in \R^{n \times k}$ y $\vecmat{B} \in \R^{k \times m}$, podemos interpretar las columnas $\vecmat{c}_{i}$, con $i = 1, \ 2 , \ \ldots, \ m$, de $\vecmat{C} \in \R^{n \times m}$ como:
\[
  \vecmat{c}_{1} = \vecmat{A} \vecmat{b}_{1}, \quad
  \vecmat{c}_{2} = \vecmat{A} \vecmat{b}_{2}, \quad \ldots,
  \vecmat{c}_{m} = \vecmat{A} \vecmat{b}_{m}
\]
Ahora volvamos a considerar a
\[
\vecmat{A}\vecmat{x} =
\begin{bmatrix}
a_{1, 1}x_{1} + a_{1, 2}x_{2} + \ldots + a_{1, m}x_{m} \\
a_{2, 1}x_{1} + a_{2, 2}x_{2} + \ldots + a_{2, m}x_{m} \\
\vdots \\
a_{n, 1}x_{1} + a_{n, 2}x_{2} + \ldots + a_{n, m}x_{m}
\end{bmatrix}
\]
El vector del lado derecho de este producto matriz-vector se puede expresar como una adición vectorial.
\[
\vecmat{A} \cdot \vecmat{x} =
\begin{bmatrix}
a_{1, 1} x_{1} \\ a_{2, 1}x_{1} \\ \vdots \\ a_{n, 1}x_{1}
\end{bmatrix} +
\begin{bmatrix}
a_{1, 2} x_{2} \\ a_{2, 2}x_{2} \\ \vdots \\ a_{n, 2}x_{2}
\end{bmatrix} +
\ldots +
\begin{bmatrix}
a_{1, m} x_{m} \\ a_{2, m}x_{m} \\ \vdots \\ a_{n, m}x_{m}
\end{bmatrix}
\]
Por otra parte, cada vector de la adición del lado derecho es una multiplicación escalar con cada componente de $\vecmat{x}$.
\[
\vecmat{A} \cdot \vecmat{x} =
x_{1} \cdot
\begin{bmatrix}
a_{1, 1} \\ a_{2, 1} \\ \vdots \\ a_{n, 1}
\end{bmatrix} +
x_{2} \cdot
\begin{bmatrix}
a_{1, 1} \\ a_{2, 2} \\ \vdots \\ a_{n, 2}
\end{bmatrix} +
\ldots +
x_{m}
\begin{bmatrix}
a_{n, 1} \\ a_{n, 2} \\ \vdots \\ a_{n, m}
\end{bmatrix}
\]
Digamos que $\vecmat{a}_{i}$ son los vectores columna de $\vecmat{A}$, entonces el lado derecho de esta igualdad se puede escribir como:
\[
  \vecmat{A} \cdot \vecmat{x} = x_{1}\vecmat{a}_{1} + x_{2}\vecmat{a}_{2} + \ldots + x_{m}\vecmat{a}_{m}
                              = \sum_{i = 1}^{m} x_{i}\vecmat{a}_{i}
\]
Esta suma que resulta del producto matriz-vector se conoce como \textbf{combinación lineal}.

De este modo, también podemos generalizar para cada columna del producto $\vecmat{C} = \vecmat{A} \cdot \vecmat{B}$ que:
\[
  \vecmat{c}_{j} = \sum_{i = 1}^{n} (b_{j, i}) \vecmat{a}_{i} \ ; \quad \text{ con } 1 \leq j \leq m
\]

\subsection{Propiedades de las operaciones con matrices.}

Suponga que es posible calcular el producto entre las matrices $\vecmat{V}, \ \vecmat{W}$ y $\vecmat{Z}$. A partir de ellas se obtienen las siguientes propiedades algebraicas.
\begin{itemize}
\item Asociativa: $\vecmat{V} \ (\vecmat{W} \vecmat{Z}) = (\vecmat{V} \vecmat{W}) \ \vecmat{Z}$
\item Distributiva: $\vecmat{V} \ (\vecmat{W} + \vecmat{Z}) = \vecmat{V}\vecmat{W} + \vecmat{V}\vecmat{Z}$ \hspace{1.5pt} y \hspace{1.5pt} $(\vecmat{W} + \vecmat{Z}) \ \vecmat{V} = \vecmat{W}\vecmat{V} + \vecmat{Z}\vecmat{V}$
\end{itemize}
La propiedad distributiva entrega el siguiente aviso: La \textbf{conmutatividad no está asegurada entre matrices}. Incluso aunque pueda calcularse el producto entre $\vecmat{V}$ y $\vecmat{W}$, suele cumplirse que
\[
  \vecmat{V} \vecmat{W} \neq \vecmat{W} \vecmat{V}
\]

\subsection{Matrices entendidas como funciones.}

Como sabemos, una función $f:X \to Y$ es una regla que asigna a cada $x \in X$ un único $y \in Y$ que es denotado como $f(x)$ (es decir, $y = f(x)$). Por ejemplo, $g:\R \to \R$ puede ser $2a + 1$ y el valor para todo $a \in \R$ es $g(a)$.

Las matrices también operan como una regla o función. Esto se puede observar en la operación producto matriz-vector.

Considere los vectores $\vecmat{v} \in V$ y $\vecmat{w} \in W$ donde $V \subseteq \R^{m}$ y $W \subseteq \R^{n}$. Podemos definir que $T:V \to W$ es $\vecmat{A} \vecmat{v}$, donde su valor de salida es $T(\vecmat{v}) = \vecmat{w}$. Es decir,
\[
  T(\vecmat{v}) = \vecmat{w} = \vecmat{A} \vecmat{v}; \quad \forall \vecmat{v} \in V
\]
Lo que indica la igualdad de arriba es que $T$ asigna a cada $\vecmat{v} \in V$ un único $\vecmat{w} \in W$. La regla (o la fórmula) que permite aquello es $\vecmat{A}$. Este tipo particular de funciones recibe el nombre de \textbf{transformación lineal}.\footnote{En matemática, la palabra ``transformación'' es un sinónimo de ``función''.}

Al trabajar con vectores, una transformación se cataloga como ``lineal'' si cumple las siguientes condiciones:

\begin{enumerate}
\item \textbf{Aditividad}: $T(\vecmat{v} + \vecmat{w}) = T(\vecmat{v}) + T(\vecmat{w})$; $\forall \vecmat{v}, \ \vecmat{w} \in V$.
\item \textbf{Homogeneidad}: $T(k \vecmat{v}) = k T(\vecmat{v})$; $\forall \vecmat{v} \in V$ y con $k =$ constante.
\end{enumerate}

La operación producto matriz-vector es una transformación lineal porque cumple con ambas condiciones.

Una composición de funciones $g \ o \ f$ es una función $h:X \to Z$ donde $f:X \to Y$ y $g:Y \to Z$, cuyo valor es $z = h(y) = g(f(x))$, para $x \in X$, $y \in Y$ y $z \in Z$. Por lo tanto, también podemos definirla como:
\[
  (g \ o \ f)(x) = g(f(x)) = z
\]
Una \textbf{composición de transformaciones lineales} sigue la misma idea vista arriba. Sean $P:U \to W$, $T:U \to V$ y $S:V \to W$, donde
\[
  T(\vecmat{u}) = \vecmat{B} \vecmat{u} \quad
  S(\vecmat{v}) = \vecmat{A} \vecmat{v}
\]
para todo $\vecmat{u} \in U$ y para todo $\vecmat{v} \in V$. Entonces:
\[
  P(\vecmat{u}) = (S \ o \ T)(\vecmat{u}) = S(T(\vecmat{u}))
\]
es una composición de transformaciones lineales.

Ahora, reemplacemos a $T(\vecmat{u}) = \vecmat{B} \vecmat{u}$ en la igualdad de arriba.
\[
  (S \ o \ T)(\vecmat{u}) = S(\vecmat{B} \vecmat{u})
\]
Como $S(\vecmat{v}) = \vecmat{A} \vecmat{v}$, entonces:
\[
  (S \ o \ T)(\vecmat{u}) = \vecmat{A}(\vecmat{B} \vecmat{u}) = (\vecmat{A} \vecmat{B}) \vecmat{u}
\]
Lo que muestra esta igualdad es que el \textbf{producto matricial} es una \textbf{composición de transformaciones lineales}.

Lo visto de manera breve en esta sección\footnote{Omití dar alguna demostración, aunque es posible encontrarlas en cualquier libro de álgebra lineal que vea transformaciones lineales.} es muy útil para entender la intuición de lo que estudiaremos en la siguiente.


\section{Matriz identidad y la inversa de una matriz.}

Dos tipos de matrices que son relevantes por sus cualidades son la matriz identidad y la inversa de una matriz. A continuación revisaremos dichas características.

\subsection{Matriz identidad.}

La \textbf{matriz identidad}, denotada como $\vecmat{I}$, es una matriz cuadrada que consiste de unos en su diagonal principal y ceros en el resto de sus entradas.
\[
\vecmat{I} =
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}
\]
Esta matriz se llama ``identidad'' porque se define como $T:V \to V$ dada por $\vecmat{I}\vecmat{v}$ la cual resulta en $\vecmat{v}$.
\[
  T(\vecmat{v}) = \vecmat{I}\vecmat{v} = \vecmat{v}
\]
En otras palabras, la regla de $\vecmat{I}$ es tomar vectores de un conjunto y devolverlos intactos. Lo mismo ocurre al multiplicar matrices
\[
  \vecmat{V} \cdot \vecmat{I} = \vecmat{V}
\]
Por este motivo, la matriz $\vecmat{I}$ se interpreta como el ``neutro multiplicativo'' del producto matricial y va en concordancia con la versión escalar de esta transformación, que corresponde a la función afín $f(x) = x$ y que es lo mismo a $f(x) = 1x$.

\subsection{Matriz Inversa.}

Una función $f:X \to Y$ es biyectiva si, para todo $x \in X$, devuelve todos los $y \in Y$ (sobreyectiva)\footnote{Lo que implica que el rango de $f$ es igual a su codominio $Y$.} los cuales son distintos entre sí (inyectiva)\footnote{La única posibilidad de que $f(x_{1}) = f(x_{2})$ es que $x_{1} = x_{2}$.}.

Cuando una función es biyectiva, puede encontrarse otra $f^{-1}:Y \to X$ cuyo valor es $f^{-1}(y) = x$ para todo $y \in Y$. Es decir, $f^{-1}$ \textbf{revierte} la regla de $f$. Ésta se conoce como la ``inversa de $f$'' y tiene la siguiente propiedad:
\[
  f^{-1}(f(x)) = x \qquad \qquad f(f^{-1}(y)) = y
\]
Por lo tanto, si $I$ es la función identidad (o afín), donde $I(x) = x$ y $I(y) = y$, entonces:
\[
  f^{-1} \ o \ f = f \ o \ f^{-1} = I
\]
Las transformaciones lineales también pueden ser biyectivas. Supongamos que $T:V \to W$ lo es donde $T(\vecmat{v}) = \vecmat{A}\vecmat{v}$, entonces existe otra $T^{-1}:W \to V$ definida como $T^{-1}(\vecmat{w}) = A^{-1}\vecmat{w}$ en la que también se cumple que:
\[
  T^{-1} \ o \ T = T \ o \ T^{-1} = I
\]
En la sección 1.4 vimos que la composición de transformaciones lineales son el producto de las matrices que las definen. Por lo tanto, podemos reescribir la igualdad de arriba como:
\[
  \invmat{A} \vecmat{A} = \vecmat{A} \invmat{A} = \vecmat{I}
\]
donde $\invmat{A}$ recibe el nombre de \textbf{matriz inversa} de $\vecmat{A}$ y la igualdad de productos de arriba se conoce como \textbf{criterio de invertibilidad}.

La inversa de una matriz suele ser una de las más buscadas al trabajar con este tipo de objetos matemáticos por las facilidades algebraicas que entrega. Por ejemplo, suponga que buscamos las soluciones de un sistema de ecuaciones lineales que hemos definido como:
\[
  \vecmat{A} \vecmat{x} = \vecmat{y}
\]
Si $\vecmat{A}$ tiene una inversa $\vecmat{A}^{-1}$ y la usamos para multiplicar a la ecuación de arriba, por el criterio de invertibilidad sucede lo siguiente:
\begin{align*}
  \invmat{A} \vecmat{A} \vecmat{x} &= \invmat{A} \vecmat{y} \\
  \vecmat{I} \vecmat{x} &= \invmat{A} \vecmat{y} \\
  \vecmat{x} &= \invmat{A} \vecmat{y}
\end{align*}
Es decir, si $\invmat{A}$ existe, la solución del sistema serán los componentes del vector $\invmat{A}\vecmat{y}$.

La gran limitante de trabajar con $\invmat{A}$ es que \textbf{solo existe en matrices cuadradas}.

Por ahora asumamos que $\vecmat{A}$ es una matriz cuadrada. Existen distintos métodos para encontrar a $\invmat{A}$. A continuación veremos uno que depende de su \textbf{determinante}, $\det(\vecmat{A})$, pero que es eficiente solo para aquellas de menor tamaño.

\subsection{Determinante de una matriz y el cálculo de su inversa.}

En la clase anterior estudiamos brevemente las fórmulas para calcular el determinante con vectores. Ahora lo haremos con matrices y veremos que su valor nos permite determinar si ésta tiene (o no) una inversa.

\subsubsection{Determinante de una matriz: Método del cofactor.}

Solo es posible conocer el determinante de matrices cuadradas. Para obtenerlo, usaremos el método del cofactor que se desarrolla a partir de matrices más chicas formadas con sus entradas. Antes de aplicarlo, debemos definir el determinante de una matriz de $2 \times 2$.

Sea $\vecmat{A} \in \R^{2 \times 2}$. Su determinante se calcula como:
\[
\det(\vecmat{A}) =
\begin{vmatrix}
a_{1, 1} & a_{1, 2} \\
a_{2, 1} & a_{2, 2}
\end{vmatrix} =
a_{1, 1}a_{2, 2} - a_{1, 2}a_{2, 1}
\]
Para calcular el determinante de matrices de $n \times n$ con $n > 2$, primero formamos bloques de matrices de $n = 2$ con sus entradas mediante lo que se conoce como \textbf{expansión de filas}. Considere la matriz $\vecmat{A} \in \R^{3 \times 3}$:
\[
\vecmat{A} =
\begin{bmatrix}
a_{1, 1} & a_{1, 2} & a_{1, 3} \\
a_{2, 1} & a_{2, 2} & a_{2, 3} \\
a_{3, 1} & a_{3, 2} & a_{3, 3}
\end{bmatrix}
\]
Luego, expandamos por la tercera\footnote{Puede ser por cualquiera de las tres.} fila de $\vecmat{A}$. Al considerar la primera columna, obtenemos la siguiente matriz $\vecmat{A}_{3, 1} \in \R^{2 \times 2}$, donde ``$3, 1$'' indica que es formada mediante la entrada $a_{3, 1}$.

\begin{figure}[hbt!]
\centering

\begin{tikzpicture}
% Líneas de ayuda.
%\draw[color = lightgray] (0, 0) grid (13, 4);

% Matrices.
\node at (3, 2) {
$
\vecmat{A} =
\begin{bmatrix}
a_{1, 1} & a_{1, 2} & a_{1, 3} \\
a_{2, 1} & a_{2, 2} & a_{2, 3} \\
a_{3, 1} & a_{3, 2} & a_{3, 3}
\end{bmatrix}
$
};
\draw[line width = 0.5mm, color = red!70] (2.5, 2.7) -- (2.5, 1.4) -- (4.7, 1.4);
\node at (5.5, 2) {$\Longrightarrow$};

\node at (7.7, 2) {
$
\vecmat{A}_{3, 1} =
\begin{bmatrix}
a_{1, 2} & a_{1, 3} \\
a_{2, 2} & a_{2, 3}
\end{bmatrix}
$
};
\end{tikzpicture}

\end{figure}

Haciendo lo mismo de arriba para $a_{3, 2}$ y $a_{3, 3}$, se obtiene lo siguiente:
\[
\vecmat{A}_{3, 2} =
\begin{bmatrix}
a_{1, 1} & a_{1, 3} \\
a_{2, 1} & a_{2, 3}
\end{bmatrix}
\quad \quad
\vecmat{A}_{3, 3} =
\begin{bmatrix}
a_{1, 1} & a_{1, 2} \\
a_{2, 1} & a_{2, 2}
\end{bmatrix}
\]
Así, los determinantes de $\vecmat{A}_{3, 1}, \ \vecmat{A}_{3, 2}$ y $\vecmat{A}_{3, 3}$, conocidos también como \textbf{menores} $M_{i, j}$, son:
\[
M_{3, 1} = 
\begin{vmatrix}
a_{1, 2} & a_{1, 3} \\
a_{2, 2} & a_{2, 3}
\end{vmatrix}
\quad \quad
M_{3, 2} = 
\begin{vmatrix}
a_{1, 1} & a_{1, 3} \\
a_{2, 1} & a_{2, 3}
\end{vmatrix}
\quad \quad
M_{3, 3} = 
\begin{vmatrix}
a_{1, 1} & a_{1, 2} \\
a_{2, 1} & a_{2, 2}
\end{vmatrix}
\]
Mediante los $M_{i, j}$ calculamos a los \textbf{cofactores} de $\vecmat{A}$ como $C_{i, j} = (-1)^{i + j} M_{i, j}$.
\begin{align*}
  C_{3, 1} &= (-1)^{3 + 1} M_{3, 1} & C_{3, 2} &= (-1)^{3 + 2} M_{3, 2} & C_{3, 3} &= (-1)^{3 + 3} M_{3, 3} \\
           &= M_{3, 1}              &          &= -(M_{3, 2})          &          &= M_{3, 3}
\end{align*}
De este modo, el determinante de $\vecmat{A}$ se calcula como la suma de los productos entre las entradas de su tercera fila y los cofactores correspondientes:
\[
  \det(\vecmat{A}) = a_{3, 1} C_{3, 1} + a_{3, 2} C_{3, 2} + a_{3, 3} C_{3, 3}
\]
Podemos generalizar esta fórmula para una matriz de $n \times n$ expandida por su $i$-ésima fila.
\[
  \det(\vecmat{A}) = \sum_{j = 1}^{n} a_{i, j} C_{i, j}; \quad \text{para } 1 \leq i \leq n
\]
También es posible obtener a $\det(\vecmat{A})$ expandiendo por las \textbf{columnas} de la matriz. En ese caso lo calculamos como:
\[
  \det(\vecmat{A}) = \sum_{i = 1}^{n} a_{i, j} C_{i, j}; \quad \text{para } 1 \leq j \leq n
\]

\subsubsection{Transpuesta de una matriz.}

La transposición de una matriz es una operación en la que se giran sus entradas con respecto a su diagonal. En otras palabras, las columnas pasan a ser sus filas y sus filas se trasladan a ser sus columnas. Por ejemplo,
\begin{align*}
\vecmat{A} &=
\begin{bmatrix}
4 & 1 & 3 \\
5 & 7 & 2
\end{bmatrix}
\Rightarrow
\transmat{A} =
\begin{bmatrix}
4 & 5 \\
1 & 7 \\
3 & 2
\end{bmatrix}
\end{align*}
donde $\transmat{A}$ es la \textbf{matriz transpuesta} de $\vecmat{A}$.

\subsubsection{Fórmula de la inversa de una matriz.}

Sea $\vecmat{C}$ la \textbf{matriz de cofactores} de $\vecmat{A} \in \R^{n \times n}$. Al transponerla obtenemos lo siguiente:
\[
  \Adj(\vecmat{A}) = \transmat{C}
\]
donde $\Adj(\vecmat{A})$ se conoce como la \textbf{matriz adjunta} de $\vecmat{A}$.

Cuando obtenemos a $\det(\vecmat{A})$ con el método de cofactores, es posible usar la fórmula de a continuación para calcular $\invmat{A}$.
\[
  \invmat{A} = \frac{1}{\det(\vecmat{A})} \cdot \Adj(\vecmat{A}) \ ; \text{ para } \det(\vecmat{A}) \neq 0
\]
Por lo tanto, $\invmat{A}$ existe sí y solo sí $\det(\vecmat{A}) \neq 0$.

Esta fórmula de $\invmat{A}$ no es recomendada para matrices de gran tamaño, ya que toma mucho tiempo obtenerla y en computadores es propensa a errores de redondeo. No obstante, la condición $\det(\vecmat{A}) \neq 0$ es la más usada para evaluar si esta matriz existe o no.

Por otra parte, la ventaja de esta fórmula de $\invmat{A}$ es su utilidad para resolver problemas algebraicos que la involucren. Los otros métodos más usados para obtenerla (como el de eliminación de Gauss) son técnicas numéricas que no permiten aquello.

\end{document}
